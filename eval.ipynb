{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5402cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in ./rerank_eval/lib/python3.10/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./rerank_eval/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Collecting dill<0.4.1,>=0.3.0\n",
      "  Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<2.0,>=0.25.0\n",
      "  Downloading huggingface_hub-1.3.2-py3-none-any.whl (534 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m534.5/534.5 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=21.0.0\n",
      "  Downloading pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (47.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.2/193.2 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./rerank_eval/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Collecting fsspec[http]<=2025.10.0,>=2023.1.0\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.0/201.0 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.32.2 in ./rerank_eval/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess<0.70.19\n",
      "  Downloading multiprocess-0.70.18-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.13.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio in ./rerank_eval/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./rerank_eval/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: certifi in ./rerank_eval/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: idna in ./rerank_eval/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./rerank_eval/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting shellingham\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting typer-slim\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hf-xet<2.0.0,>=1.2.0\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.1.0 in ./rerank_eval/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./rerank_eval/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./rerank_eval/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./rerank_eval/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./rerank_eval/lib/python3.10/site-packages (from pandas->datasets) (2025.3)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.7/241.7 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (219 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.5/219.5 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting propcache>=0.2.0\n",
      "  Downloading propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.9/196.9 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./rerank_eval/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.0/347.0 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./rerank_eval/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./rerank_eval/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Collecting click>=8.0.0\n",
      "  Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.3/108.3 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, xxhash, tqdm, shellingham, pyarrow, propcache, numpy, multidict, hf-xet, fsspec, frozenlist, filelock, dill, click, async-timeout, aiohappyeyeballs, yarl, typer-slim, pandas, multiprocess, aiosignal, aiohttp, huggingface-hub, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 async-timeout-5.0.1 click-8.3.1 datasets-4.5.0 dill-0.4.0 filelock-3.20.3 frozenlist-1.8.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-1.3.2 multidict-6.7.0 multiprocess-0.70.18 numpy-2.2.6 pandas-2.3.3 propcache-0.4.1 pyarrow-22.0.0 pytz-2025.2 shellingham-1.5.4 tqdm-4.67.1 typer-slim-0.21.1 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ac181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = '/local/scratch/tkim462/.cache/huggingface'\n",
    "os.environ['XDG_CACHE_HOME'] = '/local/scratch/tkim462/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "626c934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_QUERIES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4665d3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': ['Approximately $15,000 per year.'],\n",
       " 'passages': {'is_selected': [1, 0, 0, 0, 0, 0],\n",
       "  'passage_text': ['The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager. Salary information comes from 7,810 data points collected directly from employees, users, and jobs on Indeed.',\n",
       "   'The average revenue in 2011 of a Starbuck Store was $1,078,000, up  from $1,011,000 in 2010.    The average ticket (total purchase) at domestic Starbuck stores in  No … vember 2007 was reported at $6.36.    In 2008, the average ticket was flat (0.0% change).',\n",
       "   'In fiscal 2014, Walgreens opened a total of 184 new locations and acquired 84 locations, for a net decrease of 273 after relocations and closings. How big are your stores? The average size for a typical Walgreens is about 14,500 square feet and the sales floor averages about 11,000 square feet. How do we select locations for new stores? There are several factors that Walgreens takes into account, such as major intersections, traffic patterns, demographics and locations near hospitals.',\n",
       "   'th store in 1984, reaching $4 billion in sales in 1987, and $5 billion two years later. Walgreens ended the 1980s with 1,484 stores, $5.3 billion in revenues and $154 million in profits. However, profit margins remained just below 3 percent of sales, and returns on assets of less than 10 percent.',\n",
       "   'The number of Walgreen stores has risen from 5,000 in 2005 to more than 8,000 at present. The average square footage per store stood at approximately 10,200 and we forecast the figure to remain constant over our review period. Walgreen earned $303 as average front-end revenue per store square foot in 2012.',\n",
       "   'Your Walgreens Store. Select a store from the search results to make it Your Walgreens Store and save time getting what you need. Your Walgreens Store will be the default location for picking up prescriptions, photos, in store orders and finding deals in the Weekly Ad.'],\n",
       "  'url': ['http://www.indeed.com/cmp/Walgreens/salaries',\n",
       "   \"http://www.answers.com/Q/What_is_the_average_gross_sales_volume_of_a_single_Walgreen's_Store\",\n",
       "   'http://news.walgreens.com/fact-sheets/frequently-asked-questions.htm',\n",
       "   'http://www.babson.edu/executive-education/thought-leadership/retailing/Documents/walgreens-strategic-evolution.pdf',\n",
       "   'http://www.trefis.com/stock/wag/articles/199532/key-trends-driving-walgreens-business/2013-08-07',\n",
       "   'http://www.walgreens.com/storelocator/find.jsp?requestType=locator']},\n",
       " 'query': 'walgreens store sales average',\n",
       " 'query_id': 9652,\n",
       " 'query_type': 'numeric',\n",
       " 'wellFormedAnswers': []}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"microsoft/ms_marco\", 'v1.1')\n",
    "dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c617bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': ['$21,550 per year', 'The average hourly wage for a bartender is $10.36 and the average yearly take-home is $21,550.'], 'passages': {'is_selected': [0, 1, 0, 0, 0, 0, 0, 0], 'passage_text': ['A bartender’s income is comprised mostly of tips– 55% to be exact. In some states, employers aren’t even required to pay their bartenders the minimum wage and can pay as low as $2.13 per hour, and they depend on their tips almost entirely. Bartending can be a lot of things. For some it is exciting, for others exhausting. At times there is a lot of fun to be had, at others it is rather dull. But for the most part, bartending is almost always rewarding in the financial sense, as long as you stick with it.', 'According to the Bureau of Labor Statistics, the average hourly wage for a bartender is $10.36, and the average yearly take-home is $21,550. Bartending can be a lot of things. For some it is exciting, for others exhausting. At times there is a lot of fun to be had, at others it is rather dull. But for the most part, bartending is almost always rewarding in the financial sense, as long as you stick with it.', 'About 551,100 individuals are employed as bartenders, with half of this number working part-time. The average annual salary for bartenders is $19,050 or an equivalent of $9.16 per hour, including tips. No formal training is needed for one to get a job as a bartender as all it takes are good customer service skills and a comprehensive knowledge about beverages and recipes. ', \"Confidence votes 11. Bartenders in Vegas can make up to $7-$15 wage plus $10-$50 in tips per hour. But that totally depends on personality, gender and outlet. A hot young female can make $1000 a day tending bar in a strip club, but that is very exceptional. A bartender makes as much as a bartender wants to make. I wouldn't say there is a cap on how much a bartender make in one year. If the service is good and the coversation is i … nteresting a bartender can make alot of money. Location is also a big factor.\", 'Best Answer: An average bartender makes about...2 to 3 dollars an hour. but all the money is made off tips depending on the popularity of the bar. I used to make $700 to $1000 a night. but that is in Atlanta. If the bar is busy and you are a good bartender you will make quite a bit. I dont know how much, because I live in a town with a population of 2000 so there is not alot going on around here. Im sure the bartenders make a hundred to two hundred a night total (on a good night)...just depends.', 'Report Abuse. no way to tell you how much bartenders make. in wages anything from minimum to $15 an hour. tips, anywhere from $20 to $300 or more a night. depends on a lot of things. those top dollar jobs only come after a lot of experience. If the bar is busy and you are a good bartender you will make quite a bit. I dont know how much, because I live in a town with a population of 2000 so there is not alot going on around here. Im sure the bartenders make a hundred to two hundred a night total (on a good night)...just depends.', 'Pay by Employment Setting. Bartenders who worked in hotels and hotel restaurants generally reported the highest incomes in 2011, an average of $26,180 a year. Bartenders employed in full-service restaurants tended to earn somewhat less, averaging about $22,130 a year. Bartenders employed by bars earned an average of $20,230 per year, and bartenders who worked for civic and social organizations earned an average of $18,970 a year. The median earnings of bartenders during this period were $9.06 an hour and $18,850 a year. Eighty percent of bartenders in the U.S. reported annual incomes of between $16,170 and $31,860.', \"Tips make up half or more of bartender's salaries. If a bartender earned $6.00 an hour, their tips generally average out to $12.00 to $18.00 an hour as additional income. A bartender in an average bar will typically earn $15.00 $30.00 an hour between their wages and tips. According to bartending.org, bartenders in a high volume resort or establishment can earn $50,000 to $75,000 per year between hourly wages and tips. Indeed.com 2010 results show bartenders in restaurants at median salary rates can make a good salary per year: Bartender $73,000.\"], 'url': ['http://www.breakintobartending.com/how-much-do-bartenders-make/', 'http://www.breakintobartending.com/how-much-do-bartenders-make/', 'http://careerswiki.com/how-much-do-bartenders-make/', 'http://www.answers.com/Q/How_much_do_bartenders_in_Las_Vegas_make', 'https://answers.yahoo.com/question/index?qid=20100102233548AAZEutT', 'https://answers.yahoo.com/question/index?qid=20100102233548AAZEutT', 'http://work.chron.com/much-bartender-make-annually-7503.html', 'http://www.celebritynetworth.com/articles/how-much-does/how-much-does-bartender-make/']}, 'query': 'how much do bartenders make', 'query_id': 9653, 'query_type': 'numeric', 'wellFormedAnswers': []}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['validation'][1])\n",
    "\n",
    "if NUM_QUERIES is not None:\n",
    "    dataset['validation'] = dataset['validation'].select(range(min(NUM_QUERIES, len(dataset['validation']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de925d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prefix_prompt(query, num):\n",
    "    return [\n",
    "        {'role': 'system', 'content': \"You are RankGPT, an intelligent assistant that can rank passages based on their relevancy to the query.\"},\n",
    "        {'role': 'user', 'content': f\"I will provide you with {num} passages, each indicated by number identifier []. Rank the passages based on their relevance to query: {query}.\"},\n",
    "        {'role': 'assistant', 'content': 'Okay, please provide the passages.'}\n",
    "    ]\n",
    "\n",
    "def get_post_prompt(query, num):\n",
    "    # Simple list output rather than the > format, easier to parse \n",
    "    return (f\"Search Query: {query}. \\n\"\n",
    "            f\"Rank the {num} passages above based on their relevance to the search query. \"\n",
    "            f\"The most relevant passages should be listed first. \\n\"\n",
    "            f\"Output the ranked identifiers as a comma-separated list inside brackets, \"\n",
    "            f\"for example: [3, 0, 5, ...]. \\n\"\n",
    "            f\"Only response the ranking results, do not say any word or explain.\")\n",
    "    \n",
    "def remove_duplicate(response):\n",
    "    new_response = []\n",
    "    for c in response:\n",
    "        if c not in new_response:\n",
    "            new_response.append(c)\n",
    "    return new_response\n",
    "\n",
    "def create_ranking_prompt(item):\n",
    "    num = len(item['passages']['is_selected'])\n",
    "    query = item['query']\n",
    "    post_prompt = get_post_prompt(query, num)\n",
    "    \n",
    "    # Combine prefix, passages, and post prompt\n",
    "    messages = get_prefix_prompt(query, num)\n",
    "    for i, passage in enumerate(item['passages']['passage_text'], start=1):\n",
    "        messages.append({'role': 'user', 'content': f'[{i}] {passage}'})\n",
    "        messages.append({'role': 'assistant', 'content': f'Received passage [{i}].'})\n",
    "    messages.append({'role': 'user', 'content': post_prompt})\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0ebbcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-15 05:49:45 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
      "INFO 01-15 05:49:45 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "INFO 01-15 05:49:46 [utils.py:253] non-default args: {'disable_log_stats': True, 'model': 'meta-llama/Meta-Llama-3-8B-Instruct'}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Device string must not be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1. Initialize the model on your H200\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# For Llama-3-8B or 70B\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m----> 6\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Use tensor_parallel_size=4 or 8 for 70B if needed\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 2. Define Sampling Parameters (Greedy decoding for ranking)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m/local/scratch/tkim462/rerank_eval/lib/python3.10/site-packages/vllm/entrypoints/llm.py:351\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, structured_outputs_config, profiler_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    317\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    318\u001b[0m     runner\u001b[38;5;241m=\u001b[39mrunner,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    349\u001b[0m log_non_default_args(engine_args)\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/local/scratch/tkim462/rerank_eval/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:175\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m vllm_config \u001b[38;5;241m=\u001b[39m \u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m Executor\u001b[38;5;241m.\u001b[39mget_class(vllm_config)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m envs\u001b[38;5;241m.\u001b[39mVLLM_ENABLE_V1_MULTIPROCESSING:\n",
      "File \u001b[0;32m/local/scratch/tkim462/rerank_eval/lib/python3.10/site-packages/vllm/engine/arg_utils.py:1314\u001b[0m, in \u001b[0;36mEngineArgs.create_engine_config\u001b[0;34m(self, usage_context, headless)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;124;03mCreate the VllmConfig.\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03mNOTE: If VllmConfig is incompatible, we raise an error.\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m current_platform\u001b[38;5;241m.\u001b[39mpre_register_and_update()\n\u001b[0;32m-> 1314\u001b[0m device_config \u001b[38;5;241m=\u001b[39m \u001b[43mDeviceConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_platform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;66;03m# Check if the model is a speculator and override model/tokenizer/config\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;66;03m# BEFORE creating ModelConfig, so the config is created with the target model\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;66;03m# Skip speculator detection for cloud storage models (eg: S3, GCS) since\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;66;03m# HuggingFace cannot load configs directly from S3 URLs. S3 models can still\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;66;03m# use speculators with explicit --speculative-config.\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cloud_storage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel):\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/local/scratch/tkim462/rerank_eval/lib/python3.10/site-packages/vllm/config/device.py:75\u001b[0m, in \u001b[0;36mDeviceConfig.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Set device with device type\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Device string must not be empty"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# 1. Initialize the model on your H200\n",
    "# For Llama-3-8B or 70B\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\" \n",
    "llm = LLM(model=model_name, tensor_parallel_size=1) # Use tensor_parallel_size=4 or 8 for 70B if needed\n",
    "\n",
    "# 2. Define Sampling Parameters (Greedy decoding for ranking)\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=100)\n",
    "\n",
    "# 3. Generate prompts for a batch of MS MARCO items\n",
    "# Let's say 'dataset' is your loaded MS MARCO split\n",
    "conversations = [create_ranking_prompt(item) for item in dataset['validation']]\n",
    "\n",
    "# 4. Run Batch Inference\n",
    "outputs = llm.chat(messages=conversations, sampling_params=sampling_params, use_tqdm=True)\n",
    "\n",
    "# Extract raw text results\n",
    "raw_results = [output.outputs[0].text for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3b809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rerank_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
