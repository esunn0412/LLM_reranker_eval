# Evaluation Configuration

# Dataset Configuration
dataset:
  name: "msmarco"  # Options: "msmarco", "beir", "trec-dl"
  split: "validation"  # For MS MARCO
  num_queries: 100  # Set to null for all queries
  num_passages: 10  # Maximum passages to rerank per query (actual count may vary)
  cache_dir: "/local/scratch/tkim462/.cache/huggingface"

# Evaluation Settings
evaluation:
  # Core metrics for reranking evaluation:
  # - NDCG@k: Quality of top-k ranking (position-weighted)
  # - MAP@k: Binary relevance in top-k
  # - NDCG/MAP (full): Evaluated on all passages (adapts to actual passage count per query)
  k: 10  # Cutoff for @k metrics (set to null or large value like 100 for full ranking)
  
  # Save detailed outputs
  save_rankings: true
  save_raw_outputs: true
  
# Output Configuration
output:
  results_dir: "/local/scratch/tkim462/rerank/results"
  # Results will be saved to: {results_dir}/{model_name}/
