# Model Configuration for LLM Ranker Evaluation
# Specify which models to evaluate for reranking tasks

# Open-source LLMs (via vLLM for local inference)
open_source_models:
  - name: "llama-3b"
    model_id: "meta-llama/Llama-3.2-3B-Instruct"
    type: "vllm"
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.85
    max_tokens: 300
    temperature: 0.0
    enabled: true
    
  - name: "llama-8b"
    model_id: "meta-llama/Meta-Llama-3.1-8B-Instruct"
    type: "vllm"
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.85
    max_tokens: 300
    temperature: 0.0
    enabled: true
    
  - name: "llama-70b"
    model_id: "meta-llama/Meta-Llama-3.1-70B-Instruct"
    type: "vllm"
    tensor_parallel_size: 4  # Requires multi-GPU
    gpu_memory_utilization: 0.90
    max_tokens: 300
    temperature: 0.0
    enabled: false  # Set to true when you have 4+ GPUs
    
  - name: "gemma-27b"
    model_id: "google/gemma-2-27b-it"
    type: "vllm"
    tensor_parallel_size: 2
    gpu_memory_utilization: 0.85
    max_tokens: 300
    temperature: 0.0
    enabled: false
    
  - name: "qwen-70b"
    model_id: "Qwen/Qwen2.5-72B-Instruct"
    type: "vllm"
    tensor_parallel_size: 4
    gpu_memory_utilization: 0.90
    max_tokens: 300
    temperature: 0.0
    enabled: false

# Closed-source LLMs (via API)
closed_source_models:
  - name: "gpt-4o-mini"
    model_id: "gpt-4o-mini"
    type: "openai"
    max_tokens: 300
    temperature: 0.0
    api_key_env: "OPENAI_API_KEY"
    enabled: true
    
  - name: "gpt-4o"
    model_id: "gpt-4o"
    type: "openai"
    max_tokens: 300
    temperature: 0.0
    api_key_env: "OPENAI_API_KEY"
    enabled: false
    
  - name: "claude-sonnet-4"
    model_id: "claude-sonnet-4-20250514"
    type: "anthropic"
    max_tokens: 300
    temperature: 0.0
    api_key_env: "ANTHROPIC_API_KEY"
    enabled: false
    
  - name: "claude-haiku-4"
    model_id: "claude-4-haiku-20250514"
    type: "anthropic"
    max_tokens: 300
    temperature: 0.0
    api_key_env: "ANTHROPIC_API_KEY"
    enabled: false

# Legacy/Baseline models
baseline_models:
  - name: "gpt-3.5-turbo"
    model_id: "gpt-3.5-turbo"
    type: "openai"
    max_tokens: 300
    temperature: 0.0
    api_key_env: "OPENAI_API_KEY"
    enabled: false
